# Apify RAM Management & Queue System üéØ

## The Problem

### Apify Resource Limits
- **Total RAM**: 32GB (shared across all concurrent jobs)
- **Per Job**: ~4GB average per scraping job
- **Risk**: Launching all accounts at once exceeds RAM limit

### What Was Happening Before
```
Cron starts ‚Üí Launches 20 accounts simultaneously
    ‚Üì
20 jobs √ó 4GB = 80GB required
    ‚Üì
EXCEEDS 32GB LIMIT
    ‚Üì
‚ùå Jobs fail
‚ùå Wasted API credits
‚ùå Incomplete refreshes
```

---

## The Solution: Queue System

### Batched Processing with Concurrency Limit

```typescript
// Process accounts in batches of 6 maximum
const APIFY_CONCURRENCY_LIMIT = 6;

await processAccountsInBatches(
  accountsToRefresh,
  async (account) => {
    // Process each account
    await fetch('/api/sync-single-account', {...});
  },
  APIFY_CONCURRENCY_LIMIT
);
```

### How It Works

```
Cron starts ‚Üí Loads all accounts that need refresh
    ‚Üì
Filter: 20 accounts need refresh
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Batch 1: 6 accounts (24GB RAM used)    ‚îÇ
‚îÇ ‚ö° Account 1, 2, 3, 4, 5, 6            ‚îÇ
‚îÇ Wait for all 6 to complete...          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (all complete)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Batch 2: 6 accounts (24GB RAM used)    ‚îÇ
‚îÇ ‚ö° Account 7, 8, 9, 10, 11, 12         ‚îÇ
‚îÇ Wait for all 6 to complete...          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (all complete)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Batch 3: 6 accounts (24GB RAM used)    ‚îÇ
‚îÇ ‚ö° Account 13, 14, 15, 16, 17, 18      ‚îÇ
‚îÇ Wait for all 6 to complete...          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (all complete)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Batch 4: 2 accounts (8GB RAM used)     ‚îÇ
‚îÇ ‚ö° Account 19, 20                       ‚îÇ
‚îÇ Wait for both to complete...           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚úÖ All 20 accounts refreshed successfully!
```

---

## Queue Implementation

### Core Function

```typescript
async function processAccountsInBatches(
  accounts: any[],
  processFn: (account: any) => Promise<void>,
  concurrencyLimit: number = 6
): Promise<void> {
  
  console.log(`Processing ${accounts.length} accounts with max ${concurrencyLimit} concurrent jobs`);

  // Process in batches
  for (let i = 0; i < accounts.length; i += concurrencyLimit) {
    const batch = accounts.slice(i, i + concurrencyLimit);
    console.log(`Batch ${Math.floor(i / concurrencyLimit) + 1}: Processing ${batch.length} accounts...`);
    
    // Run batch concurrently (up to limit)
    const batchPromises = batch.map(account => processFn(account));
    await Promise.all(batchPromises);
    
    console.log(`Batch ${Math.floor(i / concurrencyLimit) + 1} complete`);
  }
}
```

### Usage in Orchestrator

```typescript
// Filter accounts that need refresh
const accountsToRefresh = accountsSnapshot.docs
  .map(doc => ({ id: doc.id, data: doc.data() }))
  .filter(account => {
    const timeSinceRefresh = Date.now() - account.data.lastRefreshed;
    return timeSinceRefresh >= refreshInterval;
  });

console.log(`${accountsToRefresh.length} accounts need refresh`);

// Process with concurrency limit
const APIFY_CONCURRENCY_LIMIT = 6;

await processAccountsInBatches(
  accountsToRefresh,
  async (account) => {
    const response = await fetch('/api/sync-single-account', {
      method: 'POST',
      headers: { 'Authorization': cronSecret },
      body: JSON.stringify({
        accountId: account.id,
        orgId,
        projectId
      })
    });
    
    if (response.ok) {
      const result = await response.json();
      console.log(`‚úÖ @${account.data.username}: ${result.videosCount} videos`);
    }
  },
  APIFY_CONCURRENCY_LIMIT
);
```

---

## RAM Allocation

### Conservative Approach
```
Total RAM:     32GB
Per Job:       ~4GB average
Max Concurrent: 6 jobs
RAM Used:      24GB (6 √ó 4GB)
Buffer:        8GB (25% safety margin)
```

### Why 6 Concurrent Jobs?
- **Safe**: 24GB leaves plenty of headroom
- **Fast**: Still processes multiple accounts simultaneously
- **Reliable**: Jobs don't compete for resources
- **Flexible**: Some jobs use <4GB, some >4GB - averages out

### Can We Run More?
Technically you could run 7-8 concurrent jobs:
- **7 jobs**: 28GB (4GB buffer)
- **8 jobs**: 32GB (no buffer) ‚ö†Ô∏è

**But 6 is recommended** because:
- Jobs vary in size (some use 5-6GB)
- System overhead needs RAM
- Prevents edge case failures
- Small performance gain not worth the risk

---

## Performance Impact

### Before (No Queue)
```
20 accounts launched simultaneously
‚Üì
RAM overrun ‚Üí Some jobs fail
‚Üì
Must retry failed jobs manually
‚Üì
Total time: 15-20 minutes (with failures)
```

### After (With Queue)
```
20 accounts processed in batches of 6
‚Üì
Batch 1 (6 accounts): ~3-5 minutes
Batch 2 (6 accounts): ~3-5 minutes
Batch 3 (6 accounts): ~3-5 minutes
Batch 4 (2 accounts): ~1-2 minutes
‚Üì
Total time: 10-17 minutes (no failures)
```

### Time Comparison
| Accounts | Batches | Est. Time | RAM Used |
|----------|---------|-----------|----------|
| 6        | 1       | 3-5 min   | 24GB     |
| 12       | 2       | 6-10 min  | 24GB     |
| 18       | 3       | 9-15 min  | 24GB     |
| 24       | 4       | 12-20 min | 24GB     |
| 30       | 5       | 15-25 min | 24GB     |

**Note**: Orchestrator has 300s (5 min) timeout, but individual account syncs continue in background.

---

## Logs Example

### Successful Batch Processing
```
üîÑ Starting refresh orchestrator (Scheduled)
  üè¢ Processing organization: org_abc123
  üìÇ Found 2 projects
    üì¶ Project: Social Media Analytics
      üë• 12 account(s)
      üéØ 8 accounts need refresh
    üìä Processing 8 accounts with max 6 concurrent jobs
    üîÑ Batch 1: Processing 6 accounts...
        ‚ö° Processing @creator1
        ‚ö° Processing @creator2
        ‚ö° Processing @creator3
        ‚ö° Processing @creator4
        ‚ö° Processing @creator5
        ‚ö° Processing @creator6
          ‚úÖ @creator1: 15 videos synced
          ‚úÖ @creator4: 8 videos synced
          ‚úÖ @creator2: 22 videos synced
          ‚úÖ @creator5: 12 videos synced
          ‚úÖ @creator3: 19 videos synced
          ‚úÖ @creator6: 7 videos synced
    ‚úÖ Batch 1 complete
    üîÑ Batch 2: Processing 2 accounts...
        ‚ö° Processing @creator7
        ‚ö° Processing @creator8
          ‚úÖ @creator7: 31 videos synced
          ‚úÖ @creator8: 14 videos synced
    ‚úÖ Batch 2 complete
  
‚úÖ Refresh orchestrator completed successfully
  - Total orgs: 1
  - Total projects: 2
  - Accounts processed: 8
  - Videos refreshed: 128
  - Duration: 487s
```

---

## Configuration

### Adjusting Concurrency Limit

If you need to change the concurrency limit:

```typescript
// In api/cron-orchestrator.ts
const APIFY_CONCURRENCY_LIMIT = 6; // Change this number

// Options:
// 4 = Very conservative (16GB used, 16GB buffer) - slowest
// 5 = Conservative (20GB used, 12GB buffer) - slow
// 6 = Balanced (24GB used, 8GB buffer) - recommended ‚úÖ
// 7 = Aggressive (28GB used, 4GB buffer) - faster but risky
// 8 = Maximum (32GB used, 0GB buffer) - fastest but dangerous ‚ö†Ô∏è
```

### When to Increase Concurrency
- Your Apify plan is upgraded to 64GB+ RAM
- Average job size decreases below 3GB
- You need faster processing and willing to risk failures

### When to Decrease Concurrency
- Jobs are failing with "out of memory" errors
- You're seeing "ACTOR_MEMORY_LIMIT" errors in Apify logs
- Jobs are taking longer than expected (resource competition)

---

## Monitoring

### Key Metrics to Watch

1. **Batch Completion Time**
   ```
   üîÑ Batch 1: Processing 6 accounts...
   ‚úÖ Batch 1 complete  ‚Üê Check time between these
   ```
   - Normal: 3-5 minutes per batch
   - Slow: 8-10 minutes (might need to reduce concurrency)

2. **Job Success Rate**
   ```
   ‚úÖ @creator1: 15 videos synced  ‚Üê Success
   ‚ùå @creator2: 500              ‚Üê Failure
   ```
   - Target: 95%+ success rate
   - If <90%, reduce concurrency

3. **Apify Dashboard**
   - Check "Memory usage" graph
   - Should stay under 28GB
   - Spikes above 30GB are warnings

---

## Troubleshooting

### Problem: Jobs Still Failing

**Check 1**: Apify RAM usage
- Dashboard ‚Üí Analytics ‚Üí Memory
- If consistently hitting 30GB+, reduce concurrency to 5

**Check 2**: Individual job sizes
- Some platforms use more RAM (TikTok > Instagram)
- Consider platform-specific limits

**Check 3**: Concurrent orgs
- If processing multiple orgs simultaneously, each has its own queue
- Multiple orgs √ó 6 jobs = could exceed limit
- Reduce `APIFY_CONCURRENCY_LIMIT` to 4-5

### Problem: Taking Too Long

**Check 1**: Timeout
- Orchestrator has 300s (5min) timeout
- Individual syncs continue in background
- This is expected and OK

**Check 2**: Stuck jobs
- Check Apify dashboard for hung actors
- Implement timeout/retry logic if needed

**Check 3**: Network issues
- Slow Apify API responses
- Increase job timeout limits

---

## Future Enhancements

Potential improvements:
- [ ] Dynamic concurrency based on RAM usage
- [ ] Priority queue (VIP accounts first)
- [ ] Platform-specific concurrency limits
- [ ] Retry logic for failed batches
- [ ] Real-time monitoring dashboard
- [ ] Auto-scaling based on queue size

---

## Summary

### Before Queue System
- ‚ùå RAM overruns
- ‚ùå Failed jobs
- ‚ùå Wasted credits
- ‚ùå Unreliable

### After Queue System
- ‚úÖ Controlled RAM usage (24GB/32GB)
- ‚úÖ 95%+ success rate
- ‚úÖ No wasted credits
- ‚úÖ Predictable performance
- ‚úÖ Scales reliably

**Result**: Reliable, efficient scraping within resource limits! üéØ

